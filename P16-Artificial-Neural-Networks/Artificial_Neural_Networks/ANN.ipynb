{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network \n",
    "\n",
    "What is a neural network? the building block of a neural network is the Neuron, we will cover the following \n",
    "in this note.\n",
    "- The Neuron\n",
    "- Activation Function\n",
    "- How do they work ? \n",
    "- How NN learns? \n",
    "- What is Gradient Descent? \n",
    "- Stochastic Gradient Descent\n",
    "- Backpropogation\n",
    "\n",
    "\n",
    "#### The Neuron : \n",
    "The whole purpose of deep learning is to mimic human brain becasue human brain is one of the most powerful learning mechanism on the planet. We first have to recreate a neuron. You can look at the oldest drawing of the nueron in this link \n",
    "https://www.the-scientist.com/foundations/the-first-neuron-drawings-1870s-34751\n",
    "\n",
    "The neuron has a body which is called the neuron, then it has Dendrites and Axons. Neuron on its own is pretty much useless,But when they work together, they can work wonders. Here is how it works, the signals from one neuron travels through its axon and reaches the dendrites of the other neuron and that's how they connect. Ok, so now let move back to technology and see how we can recreate neuroscience. In computer jargon, the neuron is a node which then receives inputs signals very much like Human brain which is like a black box that only receive input signals through your 5 senses. Then the processing occurs in the node (activation) and outputs the processed result. Output value could be continuous, categorical or binary depending on the case.\n",
    "The input values are all assigned an initial weight, and activation function looks at a weighted sum which depending on the function we choose (more later) the neuron will either pass the signal or not, we will delve deeper in a bit.\n",
    "\n",
    "Always remember to normalize or standardize your input layer before feeding it into the neuron. (check out this paper for standardization and normalization : Efficient BackProp by Yann LeCun 1998).\n",
    "\n",
    "\n",
    "#### The Activation Function\n",
    "We have many types of activation function, we can mention Threshold Function (is not smooth, has kinks but good for binary output cases cuz it returns only two values), Sigmoid function (it is smooth, it's good for probabilities), rectifier function (it's popular for ANNs), Hyperbolic Tangent tanh (values go between -1 to 1) (read more here: Deep sparse rectifier neural networks Xavier Glorot et all 2011\n",
    "http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf\n",
    ")\n",
    "\n",
    "#### How do NNs work ?\n",
    "let's look at a case for example property values. Suppose we have the following inputs: Area, Number of bedrooms, Distance to downtown of major city, age of the building. These inputs are passed with their initial weights on to the neuron which consists of hidden layers. each node in the hidden layer looks at the potential correlation between inputs for instance if the combination of area and distance both contirbute to the price then one node would pick up on that. The hidden layer picks up on the combination of inputs that have significance in determining the price. Note that we are talking about an already trained neural network.\n",
    "\n",
    "#### How do NNs learn ? \n",
    "There are two ways to go about it, you could hard code it and tell the code what to look for or provide a trained neural network and allow it to learn on its own. If you want to see which image is cat or dog you can tell the machine cat ears look like this or that or you could provide the nn with a set of cats and dogs and allow the perceptrons figure out and learn what it means to be cat or a dog. We provide those actual values, the nn's output will be measure against the actual and it will update the weights until they are as close as they can get. The learning is an iterative process basically (every time that you train your nn it is called an epoch), keep adjusting the weights and try to minimize your gradient descent. \n",
    "\n",
    "#### Gradient Descent\n",
    "In order for neural network to learn, we need to adjust our weights.  Cost function needs to minimized, we could do it through brute force and try thousands of weights and see which one minimizes the GD but the curse of dimentionality will get you in the end, so yah! not great! sometimes even sunway Taihulight won't get it! Instead lets look at downhil method or Gradient descent, basically take the slope, if slope is negative means you are going down next, if positive you're goin up, keep taking those steps until you find the optimum. This way you take fewer steps until you descent to the minimum of the cost function.\n",
    "\n",
    "#### Stochastic Gradient Descent\n",
    "There is a problem with cost functions that are not convex, we could get stuck in local minimum as opposed to the global one. In order to bypass this problem, we use stochastic gradient descent. In case of convex cost function, it's commong to use batches to adjust the weight, we take a whole batch and keep training and retraining, but the stochastic approach take one row at a time and adjusts the weights and then moves on to the next row. this way you bypass local minima in cases of non-convex cost function. you could combine the two, for instance run a few rows at a time, it's called mini batch, also useful!\n",
    "(read more Neural Network in 13 lines of Python Andrew Trask 2015)\n",
    "\n",
    "#### Backpropogation\n",
    "This is a process through which the weights are being updates, and it's a sophisticated mathematical process. The details of the math can be found in this book \"Neural Networks and Deep Learning\" by Michael Nielsen 2015. All we need to know is that the weights are updated at once.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code shows how to use artificial neural nework to predict the outcome\n",
    "#of a single ovservation. In this case, predict user churn. The performance can be \n",
    "#validated using k-fold cross validation. We tackle overfitting and drop out\n",
    "#as well as parameter tuning.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "X = dataset.iloc[:, 3:13].values\n",
    "y = dataset.iloc[:, 13].values\n",
    "\n",
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "X = X[:, 1:]\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Part 2 - Now let's make the ANN!\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)\n",
    "\n",
    "# Part 3 - Making predictions and evaluating the model\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
